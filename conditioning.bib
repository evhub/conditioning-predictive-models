@misc{post,
    author={Evan Hubinger},
    title={An overview of 11 proposals for building safe advanced {AI}},
    year=2020,
    url={https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai},
}

@article{amplification,
    author={Paul Christiano and Buck Shlegeris and Dario Amodei},
    title={Supervising strong learners by amplifying weak experts},
    journal={arXiv},
    year=2018,
    url={https://arxiv.org/abs/1810.08575},
}

@article{debate,
    author={Geoffrey Irving and Paul Christiano and Dario Amodei},
    title={{AI} safety via debate},
    journal={arXiv},
    year=2018,
    url={https://arxiv.org/abs/1805.00899},
}

@article{leike,
    author={Jan Leike and David Krueger and Tom Everitt and Miljan Martic and Vishal Maini and Shane Legg},
    title={Scalable agent alignment via reward modeling: a research direction},
    journal={arXiv},
    year=2018,
    url={https://arxiv.org/abs/1811.07871},
}

@article{risks,
    author={Evan Hubinger and Chris van Merwijk and Vladimir Mikulika and Joar Skalse and Scott Garrabrant},
    title={{Risks from Learned Optimization in Advanced Machine Learning Systems}},
    journal={arXiv},
    year=2019,
    url={https://arxiv.org/abs/1906.01820},
}

@book{superintelligence,
    author={Nick Bostrom},
    title={Superintelligence: Paths, Dangers, Strategies},
    publisher={Oxford University Press},
    year=2014,
    url={https://global.oup.com/academic/product/superintelligence-9780199678112?cc=us&lang=en&},
}

@misc{outer_alignment,
    author={Evan Hubinger},
    title={Outer alignment and imitative amplification},
    year=2020,
    url={https://www.alignmentforum.org/posts/33EKjmAdKFn3pbKPJ/outer-alignment-and-imitative-amplification},
}

@misc{market_making,
    author={Evan Hubinger},
    title={{AI} safety via market making},
    year=2020,
    url={https://www.alignmentforum.org/posts/YWwzccGbcHMJMpT45/ai-safety-via-market-making},
}

@article{tool_use,
    author={Bowen Baker and Ingmar Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},
    title={{Emergent Tool Use From Multi-Agent Autocurricula}},
    journal={arXiv},
    year=2019,
    url={https://arxiv.org/abs/1909.07528},
}

@misc{multi_agent_safety,
    author={Richard Ngo},
    title={Multi-agent safety},
    year=2020,
    url={https://www.alignmentforum.org/posts/BXMCgpktdiawT3K5v/multi-agent-safety},
}

@article{circuits,
    author={Chris Olah and Nick Cammarata and Ludwig Schubert and Gabriel Goh and Michael Petrov and Shan Carter},
    title={{Thread: Circuits}},
    journal={Distill},
    year=2020,
    url={https://distill.pub/2020/circuits/},
}

@misc{catastrophes,
    author={Paul Christiano},
    title={Learning with catastrophes},
    year=2016,
    url={https://ai-alignment.com/learning-with-catastrophes-59387b55cc30},
}

@misc{chris_olah,
    author={Evan Hubinger},
    title={{Chris Olah’s views on AGI safety}},
    year=2019,
    url={https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety},
}

@misc{adversarial_ida,
    author={Evan Hubinger},
    title={{A Concrete Proposal for Adversarial IDA}},
    year=2019,
    url={https://www.alignmentforum.org/posts/jYvm4mmjvGHcPXtGL/a-concrete-proposal-for-adversarial-ida},
}

@misc{strong_hch,
    author={Paul Christiano},
    title={Strong {HCH}},
    year=2016,
    url={https://ai-alignment.com/strong-hch-bedb0dc08d4e},
}

@misc{universality,
    author={Paul Christiano},
    title={Universality and consequentialism within {HCH}},
    year=2019,
    url={https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd},
}

@misc{mechanistic,
    author={Evan Hubinger},
    title={Towards a mechanistic understanding of corrigibility},
    year=2019,
    url={https://www.alignmentforum.org/posts/BKM8uQS6QdJPZLqCr/towards-a-mechanistic-understanding-of-corrigibility},
}

@misc{efficient_feedback,
    author={Paul Christiano},
    title={Efficient feedback},
    year=2015,
    url={https://ai-alignment.com/efficient-feedback-a347748b1557},
}

@misc{relaxed,
    author={Evan Hubinger},
    title={Relaxed adversarial training for inner alignment},
    year=2019,
    url={https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment},
}

@misc{gradient_hacking,
    author={Evan Hubinger},
    title={Gradient hacking},
    year=2019,
    url={https://www.alignmentforum.org/posts/uXH4r6MmKPedk8rMA/gradient-hacking},
}

@article{deep_tamer,
    author={Garrett Warnell and Nicholas Waytowich and Vernon Lawhern and Peter Stone},
    title={{Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces}},
    journal={arXiv},
    year=2017,
    url={https://arxiv.org/abs/1709.10163},
}


@article{deep_rl,
    author={Dilip Arumugam and Jun Ki Lee and Sophie Saskin and Michael L. Littman},
    title={{Deep Reinforcement Learning from Policy-Dependent Human Feedback}},
    journal={arXiv},
    year=2019,
    url={https://arxiv.org/abs/1902.04257},
}

@misc{model_free,
    author={Paul Christiano},
    title={Approval-directed agents},
    year=2014,
    url={https://ai-alignment.com/model-free-decisions-6e6609f5d99e},
}

@misc{visualizing,
    author={Chris Olah},
    title={{Visualizing Representations: Deep Learning and Human Beings}},
    year=2015,
    url={https://colah.github.io/posts/2015-01-Visualizing-Representations/},
}

@misc{universal_prior,
    author={Paul Christiano},
    title={What does the universal prior actually look like?},
    year=2016,
    url={https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like},
}

@misc{partial_agency,
    author={Abram Demski},
    title={{Partial Agency}},
    year=2019,
    url={https://www.alignmentforum.org/s/HeYtBkNbEe7wpjc6X},
}

@article{language_models,
    author={Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
    title={{Language Models are Unsupervised Multitask Learners}},
    journal={{OpenAI}}, 
    year=2019,
    url={https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
}

@article{human_models,
    author={Ramana Kumar and Scott Garrabrant},
    title={Thoughts on Human Models},
    journal={MIRI},
    year=2019,
    url={https://intelligence.org/2019/02/22/thoughts-on-human-models},
}

@article{theorem_proving,
    author={Mitsuru Kusumoto and Keisuke Yahata and Masahiro Sakai},
    title={{Automated Theorem Proving in Intuitionistic Propositional Logic by Deep Reinforcement Learning}},
    journal={arXiv},
    year=2018,
    url={https://arxiv.org/abs/1811.00796},
}

@article{holist,
    author={Kshitij Bansal and Sarah M. Loos and Markus N. Rabe and Christian Szegedy and Stewart Wilcox},
    title={{HOList: An Environment for Machine Learning of Higher-Order Theorem Proving}},
    journal={arXiv},
    year=2019,
    url={https://arxiv.org/abs/1904.03241},
}

@article{protein,
    author={Andrew W. Senior and Richard Evans and John Jumper and James Kirkpatrick and Laurent Sifre and Tim Green and Chongli Qin and Augustin Žídek and Alexander W. R. Nelson and Alex Bridgland and Hugo Penedones and Stig Petersen and Karen Simonyan and Steve Crossan and Pushmeet Kohli and David T. Jones and David Silver and Koray Kavukcuoglu and Demis Hassabis},
    title={Improved protein structure prediction using potentials from deep learning},
    journal={Nature},
    year=2020,
    url={https://www.nature.com/articles/s41586-019-1923-7.epdf},
}

@article{vulnerable,
    author={Nick Bostrom},
    title={{The Vulnerable World Hypothesis}},
    journal={{Global Policy}},
    year=2019,
    url={https://nickbostrom.com/papers/vulnerable.pdf},
}

@article{wbe,
    author={Anders Sandberg and Nick Bostrom},
    title={{Whole Brain Emulation: A Roadmap}},
    journal={{FHI}},
    year=2008,
    url={https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf},
}

@article{out_of_distribution,
    author={Jie Ren and Peter J. Liu and Emily Fertig and Jasper Snoek and Ryan Poplin and Mark A. DePristo and Joshua V. Dillon and Balaji Lakshminarayanan},
    title={{Likelihood Ratios for Out-of-Distribution Detection}},
    journal={arXiv},
    year=2019,
    url={https://arxiv.org/abs/1906.02845},
}

@article{generative,
    author={Jonathan Ho and Stefano Ermon},
    title={{Generative Adversarial Imitation Learning}},
    journal={arXiv},
    year=2016,
    url={https://arxiv.org/abs/1606.03476},
}

@article{learning_robust_rewards,
    author={Justin Fu and Katie Luo and Sergey Levine},
    title={{Learning Robust Rewards with Adversarial Inverse Reinforcement Learning}},
    journal={arXiv},
    year=2017,
    url={https://arxiv.org/abs/1710.11248},
}

@article{reframing_si,
    author={K. Eric Drexler},
    title={{Reframing Superintelligence: Comprehensive AI Services as General Intelligence}},
    journal={{FHI}},
    year=2019,
    url={https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf},
}

@misc{debate_progress,
    author={Beth Barnes and Paul Christiano},
    title={{Writeup: Progress on AI Safety via Debate}},
    year=2020,
    url={https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1},
}

@article{go,
    author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
    title={A general reinforcement learning algorithm that masters chess, shogi, and {Go} through self-play},
    journal={Science},
    year=2018,
    url={https://science.sciencemag.org/content/362/6419/1140.full?ijkey=XGd77kI6W4rSc&keytype=ref&siteid=sci},
}

@misc{openai_five,
    author={Filip Wolski and Szymon Sidor and Michael Petrov and David Farhi and Jonathan Raiman and Susan Zhang and Greg Brockman and Christy Dennison and Jie Tang and Henrique Pondé and Brooke Chan and Jakub Pachocki and Przemysław Dębiak},
    title={{OpenAI Five}},
    year=2018,
    url={https://openai.com/blog/openai-five/},
}

@misc{alphastar,
    author={{The AlphaStar team}},
    title={{AlphaStar: Mastering the Real-Time Strategy Game StarCraft II}},
    year=2019,
    url={https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii},
}

@misc{synthesizing,
    author={Evan Hubinger},
    title={Synthesizing amplification and debate},
    year=2020,
    url={https://www.alignmentforum.org/posts/dJSD5RK6Qoidb3QY5/synthesizing-amplification-and-debate},
}








%below are not actually cited in this paper

@misc{bottle_caps,
    author={Daniel Filan},
    title={Bottle Caps Aren't Optimisers},
    year=2018,
    url={http://danielfilan.com/2018/08/31/bottle_caps_arent_optimisers.html},
}

@article{treeqn,
    title={{TreeQN} and {ATreeC}: Differentiable Tree-Structured Models for Deep Reinforcement Learning},
    author={Farquhar, Gregory and Rockt{\"a}schel, Tim and Igl, Maximilian and Whiteson, Shimon},
    journal={ICLR 2018},
    year=2018,
    url={https://arxiv.org/abs/1710.11417},
}

@article{univ_plan_net,
    title={Universal Planning Networks},
    author={Aravind Srinivas and Allan Jabri and Pieter Abbeel and Sergey Levine and Chelsea Finn},
    journal={ICML 2018},
    year=2018,
    url={https://arxiv.org/abs/1804.00645},
}

@article{grad_by_grad,
    title={Learning to learn by gradient descent by gradient descent},
    author={Marcin Andrychowicz and Misha Denil and Sergio Gomez and Matthew W. Hoffman and David Pfau and Tom Schaul and Brendan Shillingford and Nando de Freitas},
    journal={NIPS 2016},
    year=2016,
    url={https://arxiv.org/abs/1606.04474},
}

@article{rl2,
    author={Yan Duan and John Schulman and Xi Chen and Peter L. Bartlett and Ilya Sutskever and Pieter Abbeel},
    title={{RL}$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning},
    journal={arXiv},
    year=2016,
    url={https://arxiv.org/abs/1611.02779},
}

@misc{arbital_daemons,
    author={Eliezer Yudkowsky},
    title={Optimization daemons},
    url={https://arbital.com/p/daemons},
}

@article{mesa,
    author={Joe Cheal},
    title={What is the Opposite of Meta?},
    journal={ANLP Acuity Vol. 2},
    url={http://www.gwiznlp.com/wp-content/uploads/2014/08/Whats-the-opposite-of-meta.pdf},
}



@misc{optpow,
    author={Eliezer Yudkowsky},
    title={Measuring Optimization Power},
    year=2008,
    url={https://www.lesswrong.com/posts/Q4hLMDrFd8fbteeZ8/measuring-optimization-power},
}

@article{alphazero,
    author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
    title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
    journal={Science},
    volume=362,
    number=6419,
    pages={1140--1144},
    year=2018,
    url={https://science.sciencemag.org/content/362/6419/1140.full},
}

@article{drexler,
    author={K. E. Drexler},
    title={Reframing Superintelligence: Comprehensive AI Services as General Intelligence},
    journal={Technical Report \#2019-1, Future of Humanity Institute, University of Oxford},
    year=2019,
    url={https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf},
}


@misc{paul_solomonoff,
    author={Paul Christiano},
    title={What does the universal prior actually look like?},
    year=2016,
    url={https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like},
}

@article{neural_tms,
    author={Alex Graves and Greg Wayne and Ivo Danihelka},
    title={Neural Turing Machines},
    journal={arXiv},
    year=2014,
    url={https://arxiv.org/abs/1410.5401},
}

@article{nn_simp_bias,
    author={Guillermo Valle-Pérez and Chico Q. Camargo and Ard A. Louis},
    title={Deep learning generalizes because the parameter-function map is biased towards simple functions},
    journal={ICLR 2019},
    year=2019,
    url={https://arxiv.org/abs/1805.08522},
}

@misc{paul_minimal_circuits,
    author={Paul Christiano},
    title={Open question: are minimal circuits daemon-free?},
    year=2018,
    url={https://www.lesswrong.com/posts/nyCHnY7T5PHPLjxmN/open-question-are-minimal-circuits-daemon-free},
}

@misc{chris,
    author={Chris van Merwijk},
    title={Development of {AI} agents as a principal-agent problem},
    year={Forthcoming in 2019},
}

@article{ibarz,
    author={Borja Ibarz and Jan Leike and Tobias Pohlen and Geoffrey Irving and Shane Legg and Dario Amodei},
    title={Reward learning from human preferences and demonstrations in {Atari}},
    journal={NeurIPS 2018},
    year=2018,
    url={https://arxiv.org/abs/1811.06521},
}

@article{adversarial_examples,
    author={Jiawei Su and Danilo Vasconcellos Vargas and Kouichi Sakurai},
    title={One pixel attack for fooling deep neural networks},
    journal={IEEE Transactions on Evolutionary Computation},
    year=2017,
    url={http://arxiv.org/abs/1710.08864},
}

@article{irl_unidentifiability,
    author={Kareem Amin and Satinder Singh},
    title={Towards Resolving Unidentifiability in Inverse Reinforcement Learning},
    journal={arXiv},
    year=2016,
    url={https://arxiv.org/abs/1601.06569},
}

@article{imagination_planners,
    author={Razvan Pascanu and Yujia Li and Oriol Vinyals and Nicolas Heess and Lars Buesing and Sebastien Racanière and David Reichert and Théophane Weber and Daan Wierstra and Peter Battaglia},
    title={Learning model-based planning from scratch},
    journal={arXiv},
    year=2017,
    url={https://arxiv.org/abs/1707.06170},
}



@article{goodhart,
    author={David Manheim and Scott Garrabrant},
    title={Categorizing Variants of {Goodhart's} Law},
    journal={arXiv},
    year=2018,
    url={https://arxiv.org/abs/1803.04585},
}


@misc{paul_doom,
    author={Paul Christiano},
    title={What failure looks like},
    year=2019,
    url={https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/more-realistic-tales-of-doom},
}

@article{corrigibility,
    author={Nate Soares and Benja Fallenstein and Eliezer Yudkowsky and Stuart Armstrong},
    title={Corrigibility},
    journal={AAAI 2015},
    year=2015,
    url={https://intelligence.org/files/Corrigibility.pdf},
}

@misc{paul_robust_corrigibility,
    author={Paul Christiano},
    title={Worst-case guarantees},
    year=2019,
    url={https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d},
}

@article{absent_minded_driver,
    author={Robert J. Aumann and Sergiu Hart and Motty Perry},
    title={The Absent-Minded Driver},
    journal={Games and Economic Behavior},
    volume=20,
    pages={102--116},
    year=1997,
    url={http://www.ma.huji.ac.il/raumann/pdf/Minded\%20Driver.pdf},
}



@article{learn_to_rl,
    author={Jane X Wang and Zeb Kurth-Nelson and Dhruva Tirumala and Hubert Soyer and Joel Z Leibo and Remi Munos and Charles Blundell and Dharshan Kumaran and Matt Botvinick},
    title={Learning to reinforcement learn},
    journal={CogSci},
    year=2016,
    url={https://arxiv.org/abs/1611.05763},
}

@article{concrete_problems,
    author={Dario Amodei and Chris Olah and Jacob Steinhardt and Paul Christiano and John Schulman and Dan Mané},
    title={Concrete Problems in {AI} Safety},
    journal={arXiv},
    year=2016,
    url={https://arxiv.org/abs/1606.06565},
}

@article{armstrong_preferences,
    author={Stuart Armstrong and Sören Mindermann},
    title={Occam's razor is insufficient to infer the preferences of irrational agents},
    journal={NeurIPS 2018},
    year=2017,
    url={https://arxiv.org/abs/1712.05812},
}

@article{safety_verification,
    author={Xiaowei Huang and Marta Kwiatkowska and Sen Wang and Min Wu},
    title={{Safety Verification of Deep Neural Networks}},
    journal={CAV 2017},
    year=2016,
    url={https://arxiv.org/abs/1610.06940},
}

@article{reluplex,
    author={Guy Katz and Clark Barrett and David Dill and Kyle Julian and Mykel Kochenderfer},
    title={{Reluplex: An Efficient {SMT} Solver for Verifying Deep Neural Networks}},
    journal={CAV 2017},
    year=2017,
    url={https://arxiv.org/abs/1702.01135},
}

@article{practical_verification,
    author={Kexin Pei and Yinzhi Cao and Junfeng Yang and Suman Jana},
    title={{Towards Practical Verification of Machine Learning: The Case of Computer Vision Systems}},
    journal={arXiv},
    year=2017,
    url={https://arxiv.org/abs/1712.01785},
}

@misc{lesswrong_daemons,
    author={Riceissa},
    title={Optimization daemons},
    year=2018,
    url={https://wiki.lesswrong.com/wiki/Optimization_daemons},
}

@misc{induction-heads,
    author={Catherine Olsson and Nelson Elhage and Neel Nanda and Nicholas Joseph and Nova DasSarma and Tom Henighan and 
    Ben Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and 
    Danny Hernandez and Scott Johnston and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and 
    Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah},
    title={In-context Learning and Induction Heads},
    year=2022,
    url={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html},
}

@article{elk,
	title = {Eliciting latent knowledge: How to tell if your eyes deceive you},
	shorttitle = {Eliciting latent knowledge},
	url = {https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge},
	abstract = {ARC has published a report on Eliciting Latent Knowledge, an open problem which we believe is central to alignment. We think reading this report is the clearest way to understand what problems we are…},
	language = {en},
	year = {2021},
	author = {Paul Christiano and Mark Xu and Ajeya Cotra},
}


@article{simulators,
	title = {Simulators},
	url = {https://www.alignmentforum.org/posts/vJFdjigzmcXMhNTsx/simulators#:~:text=In%20general%2C%20the,the%20model%E2%80%99s%20power},
	language = {en},
	year = {2022},
	author = {Janus},
}

@article{how_become_confident,
	title = {How do we become confident in the safety of a machine learning system?},
	url = {https://www.alignmentforum.org/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine},
	language = {en},
	year=2021,
	author = {Hubinger, Evan},
}

@misc{wikipedia_bayesian_network,
	title = {Bayesian network},
	url = {https://en.wikipedia.org/w/index.php?title=Bayesian_network&oldid=1134656115},
	journal = {Wikipedia},
	month = jan,
	year = {2023},
	note = {Page Version ID: 1134656115},
}

@article{lms_multiverse_generators,
	title = {Language models are multiverse generators},
	url = {https://generative.ink/posts/language-models-are-multiverse-generators/},
	language = {en},
	year = {2021},
	author = {Janus},
}

@misc{kl_penalty,
    author={Tomek Korbak and Ethan Perez},
    title={RL with KL penalties is better seen as Bayesian inference},
    year=2022,
    url={https://www.lesswrong.com/posts/eoHbneGvqDu25Hasc/rl-with-kl-penalties-is-better-seen-as-bayesian-inference},
}

@misc{multiple_worlds,
    author={Evan Hubinger},
    title={Multiple Worlds, One Universal Wave Function},
    year=2020,
    url={https://www.lesswrong.com/posts/2D9s6kpegDQtrueBE/multiple-worlds-one-universal-wave-function},
}

@techreport{deep_RL_human_pref,
	title = {Deep reinforcement learning from human preferences},
	url = {http://arxiv.org/abs/1706.03741},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals deﬁned in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1\% of our agent’s interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the ﬂexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
	language = {en},
	number = {arXiv:1706.03741},
	urldate = {2023-01-31},
	institution = {arXiv},
	author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
	month = jul,
	year = {2017},
	note = {arXiv:1706.03741 [cs, stat]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Human-Computer Interaction},
	file = {Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf:/home/sara/Zotero/storage/2QH3C2QG/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf:application/pdf},
}

@techreport{chain_of_thought,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—signiﬁcantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufﬁciently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even ﬁnetuned GPT-3 with a veriﬁer.},
	language = {en},
	number = {arXiv:2201.11903},
	urldate = {2023-01-31},
	institution = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:/home/sara/Zotero/storage/KU6W2I89/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf},
}

@misc{how_likely_deception,
    author={Evan Hubinger},
    title={How likely is deceptive alignment?},
    year=2022,
    url={https://www.alignmentforum.org/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment},
}


@misc{aligning_lms_follow_instructions,
	title = {Aligning {Language} {Models} to {Follow} {Instructions}},
	url = {https://openai.com/blog/instruction-following/},
	abstract = {We've trained language models that are much better at following user intentions
than GPT-3 while also making them more truthful and less toxic, using techniques
developed through our alignment research. These InstructGPT models, which are
trained with humans in the loop, are now deployed as the default language models},
	language = {en},
	urldate = {2023-01-31},
	journal = {OpenAI},
	month = jan,
	year = {2022},
	file = {Snapshot:/home/sara/Zotero/storage/45MRS2GU/instruction-following.html:text/html},
}
